# ЁЯзо р╕Ър╕Чр╕Чр╕╡р╣И 8 р╕Бр╕▓р╕гр╣Ар╕Юр╕┤р╣Ир╕бр╕Ыр╕гр╕░р╕кр╕┤р╕Чр╕Шр╕┤р╕ар╕▓р╕Юр╣Вр╕бр╣Ар╕Фр╕е (Model Optimization)
> р╕кр╕гр╕╕р╕Ыр╕Юр╕гр╣Йр╕нр╕бр╕зр╕┤р╕Шр╕╡р╣Гр╕Кр╣Йр╕кр╕╣р╕Хр╕гр╣Бр╕ер╕░р╕Бр╕▓р╕гр╣Бр╕Бр╣Йр╣Вр╕Ир╕Чр╕вр╣Мр╕нр╕вр╣Ир╕▓р╕Зр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Ф

---

## ЁЯУШ р╣Бр╕Щр╕зр╕Др╕┤р╕Фр╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ
р╕Бр╕▓р╕гр╣Ар╕Юр╕┤р╣Ир╕бр╕Ыр╕гр╕░р╕кр╕┤р╕Чр╕Шр╕┤р╕ар╕▓р╕Юр╣Вр╕бр╣Ар╕Фр╕е (Model Optimization) р╕Др╕╖р╕нр╕Бр╕▓р╕гр╕лр╕▓р╕Др╣Ир╕▓р╕Юр╕▓р╕гр╕▓р╕бр╕┤р╣Ар╕Хр╕нр╕гр╣Мр╕Вр╕нр╕Зр╕кр╕бр╕Бр╕▓р╕гр╕Чр╕╡р╣Ир╕Чр╕│р╣Гр╕лр╣Й "р╕Др╕зр╕▓р╕бр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╕гр╕зр╕б" р╕Щр╣Йр╕нр╕вр╕Чр╕╡р╣Ир╕кр╕╕р╕Ф  
р╣Гр╕Щр╣Ар╕Кр╕┤р╕Зр╕Др╕Ур╕┤р╕Хр╕ир╕▓р╕кр╕Хр╕гр╣М р╣Ар╕гр╕▓р╕Юр╕вр╕▓р╕вр╕▓р╕бр╕Чр╕│р╣Гр╕лр╣Йр╕Яр╕▒р╕Зр╕Бр╣Мр╕Кр╕▒р╕Щ **Loss Function** р╕бр╕╡р╕Др╣Ир╕▓р╕Щр╣Йр╕нр╕вр╕Чр╕╡р╣Ир╕кр╕╕р╕Ф

---

## ЁЯзй р╕кр╣Ир╕зр╕Щр╕Чр╕╡р╣И 1 тАФ Least Squares Method (р╕зр╕┤р╕Шр╕╡р╕Бр╕│р╕ер╕▒р╕Зр╕кр╕нр╕Зр╕Щр╣Йр╕нр╕вр╕Чр╕╡р╣Ир╕кр╕╕р╕Ф)

### ЁЯФ╣ р╣Бр╕Щр╕зр╕Др╕┤р╕Ф
р╕лр╕▓р╣Ар╕кр╣Йр╕Щр╕Хр╕гр╕З (Line of Best Fit) р╕Чр╕╡р╣Ир╕Чр╕│р╣Гр╕лр╣Йр╕гр╕░р╕вр╕░р╕лр╣Ир╕▓р╕Зр╕гр╕░р╕лр╕зр╣Ир╕▓р╕Зр╕Ир╕╕р╕Фр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕гр╕┤р╕Зр╕Бр╕▒р╕Ър╣Ар╕кр╣Йр╕Щр╕Хр╕гр╕З тАЬр╕гр╕зр╕бр╕Бр╕▒р╕Щр╣Бр╕ер╣Йр╕зр╕Щр╣Йр╕нр╕вр╕Чр╕╡р╣Ир╕кр╕╕р╕ФтАЭ  

### ЁЯФ╣ р╕кр╕бр╕Бр╕▓р╕гр╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ

$$
y_i = b_0 + b_1 x_i + \epsilon_i
$$

р╣Ар╕гр╕▓р╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕гр╕лр╕▓р╕Др╣Ир╕▓ $b_0$ (intercept) р╣Бр╕ер╕░ $b_1$ (slope)  
р╣Вр╕Фр╕вр╕ер╕Фр╕Др╣Ир╕▓ **SSE (Sum of Squared Errors)**

$$
SSE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 = \sum_{i=1}^{n} (y_i - b_0 - b_1x_i)^2
$$

---

### ЁЯза р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Бр╕▓р╕гр╣Бр╕Бр╣Йр╣Вр╕Ир╕Чр╕вр╣Мр╕Чр╕╡р╕ер╕░р╕Вр╕▒р╣Йр╕Щ
#### р╕Вр╕▒р╣Йр╕Щр╕Чр╕╡р╣И 1: р╕лр╕▓ $\bar{x}$ р╣Бр╕ер╕░ $\bar{y}$
$$
\bar{x} = \frac{\sum x_i}{n}, \quad \bar{y} = \frac{\sum y_i}{n}
$$

#### р╕Вр╕▒р╣Йр╕Щр╕Чр╕╡р╣И 2: р╕лр╕▓ $S_{xx}$ р╣Бр╕ер╕░ $S_{xy}$
$$
S_{xx} = \sum (x_i - \bar{x})^2, \quad S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y})
$$

#### р╕Вр╕▒р╣Йр╕Щр╕Чр╕╡р╣И 3: р╕лр╕▓ $b_1$ р╣Бр╕ер╕░ $b_0$
$$
b_1 = \frac{S_{xy}}{S_{xx}}, \quad b_0 = \bar{y} - b_1 \bar{x}
$$

#### р╕Вр╕▒р╣Йр╕Щр╕Чр╕╡р╣И 4: р╣Ар╕Вр╕╡р╕вр╕Щр╕кр╕бр╕Бр╕▓р╕гр╣Ар╕кр╣Йр╕Щр╕Хр╕гр╕З
$$
\hat{y} = b_0 + b_1x
$$

---

### ЁЯТб р╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╣Вр╕Ир╕Чр╕вр╣Мр╕Чр╕╡р╣И 1 тАФ Least Squares

| X | Y |
|:-:|:-:|
| 1 | 2 |
| 2 | 4 |
| 3 | 5 |
| 4 | 4 |
| 5 | 5 |

#### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 1: р╕лр╕▓р╕Др╣Ир╕▓р╣Ар╕Йр╕ер╕╡р╣Ир╕в
$$
\bar{x} = \frac{1+2+3+4+5}{5} = 3, \quad \bar{y} = \frac{2+4+5+4+5}{5} = 4
$$

#### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 2: р╕Др╕│р╕Щр╕зр╕У $S_{xx}$ р╣Бр╕ер╕░ $S_{xy}$

| X | Y | $(x_i - \bar{x})$ | $(y_i - \bar{y})$ | $(x_i - \bar{x})(y_i - \bar{y})$ | $(x_i - \bar{x})^2$ |
|:-:|:-:|:-:|:-:|:-:|:-:|
| 1 | 2 | -2 | -2 | 4 | 4 |
| 2 | 4 | -1 | 0 | 0 | 1 |
| 3 | 5 | 0 | 1 | 0 | 0 |
| 4 | 4 | 1 | 0 | 0 | 1 |
| 5 | 5 | 2 | 1 | 2 | 4 |

$$
S_{xx} = 10, \quad S_{xy} = 6
$$

#### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 3: р╕лр╕▓р╕Др╣Ир╕▓ $b_1$ р╣Бр╕ер╕░ $b_0$
$$
b_1 = \frac{S_{xy}}{S_{xx}} = \frac{6}{10} = 0.6
$$
$$
b_0 = \bar{y} - b_1 \bar{x} = 4 - (0.6)(3) = 2.2
$$

#### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 4: р╕кр╕бр╕Бр╕▓р╕гр╣Ар╕кр╣Йр╕Щр╕Хр╕гр╕З
$$
\hat{y} = 2.2 + 0.6x
$$

---

### ЁЯФН р╕Бр╕▓р╕гр╕Ыр╕гр╕░р╣Ар╕бр╕┤р╕Щр╕Ьр╕ер╕Фр╣Йр╕зр╕в $R^2$
$$
R^2 = 1 - \frac{SSE}{SST}
$$
р╣Вр╕Фр╕вр╕Чр╕╡р╣И  
$$
SST = \sum (y_i - \bar{y})^2
$$

#### р╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╕Др╕│р╕Щр╕зр╕У
$$
SST = 6, \quad SSE = 2.4
$$
$$
R^2 = 1 - \frac{2.4}{6} = 0.6
$$
р╣Вр╕бр╣Ар╕Фр╕ер╕нр╕Шр╕┤р╕Ър╕▓р╕вр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Др╕Фр╣Й 60%

---

## тЪЩя╕П р╕кр╣Ир╕зр╕Щр╕Чр╕╡р╣И 2 тАФ Gradient Descent (р╕зр╕┤р╕Шр╕╡р╕ер╕Фр╕Др╕зр╕▓р╕бр╕Кр╕▒р╕Щ)

### ЁЯФ╣ р╣Бр╕Щр╕зр╕Др╕┤р╕Ф
р╣Бр╕Чр╕Щр╕Чр╕╡р╣Ир╕Ир╕░р╣Гр╕Кр╣Йр╕кр╕╣р╕Хр╕гр╕Хр╕гр╕З (р╣Бр╕Ър╕Ър╕Ыр╕┤р╕Ф) р╣Ар╕гр╕▓р╣Гр╕Кр╣Йр╕Бр╕▓р╕г "р╕ер╕нр╕Зр╕Ыр╕гр╕▒р╕Ър╕Др╣Ир╕▓" р╕Юр╕▓р╕гр╕▓р╕бр╕┤р╣Ар╕Хр╕нр╕гр╣Мр╕Лр╣Йр╕│ р╣Ж р╣Ар╕Юр╕╖р╣Ир╕нр╣Гр╕лр╣Й Loss р╕ер╕Фр╕ер╕Зр╣Ар╕гр╕╖р╣Ир╕нр╕в р╣Ж

---

### ЁЯФ╣ р╕кр╕бр╕Бр╕▓р╕гр╕лр╕ер╕▒р╕Б
$$
b_j := b_j - \alpha \frac{\partial L}{\partial b_j}
$$

р╣Вр╕Фр╕в  
- $b_j$ = р╕Юр╕▓р╕гр╕▓р╕бр╕┤р╣Ар╕Хр╕нр╕гр╣М (р╕нр╕▓р╕Ир╣Ар╕Ыр╣Зр╕Щ $b_0$ р╕лр╕гр╕╖р╕н $b_1$)  
- $\alpha$ = Learning rate  
- $\frac{\partial L}{\partial b_j}$ = р╕нр╕Щр╕╕р╕Юр╕▒р╕Щр╕Шр╣Мр╕Вр╕нр╕З Loss Function  

р╣Гр╕Щ Linear Regression:
$$
L = \sum_{i=1}^{n}(y_i - (b_0 + b_1x_i))^2
$$

р╕нр╕Щр╕╕р╕Юр╕▒р╕Щр╕Шр╣М:
$$
\frac{\partial L}{\partial b_0} = -2 \sum (y_i - b_0 - b_1x_i)
$$
$$
\frac{\partial L}{\partial b_1} = -2 \sum x_i (y_i - b_0 - b_1x_i)
$$

---

## ЁЯзй р╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╣Вр╕Ир╕Чр╕вр╣Мр╕Чр╕╡р╣И 2 тАФ Gradient Descent

р╣Гр╕лр╣Йр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Ар╕лр╕бр╕╖р╕нр╕Щр╣Ар╕Фр╕┤р╕б:  
$(X,Y) = \{(1,2),(2,4),(3,5),(4,4),(5,5)\}$  
р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╣Ар╕гр╕┤р╣Ир╕бр╕Хр╣Йр╕Щ:  
$b_0^{(0)} = 0, \quad b_1^{(0)} = 0, \quad \alpha = 0.05$

---

### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 1: р╕Др╕│р╕Щр╕зр╕Ур╕Др╣Ир╕▓р╕Чр╕│р╕Щр╕▓р╕вр╣Ар╕гр╕┤р╣Ир╕бр╕Хр╣Йр╕Щ
$$
\hat{y_i} = b_0 + b_1x_i = 0
$$
Residual:
$$
e_i = y_i - \hat{y_i} = [2, 4, 5, 4, 5]
$$

---

### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 2: р╕Др╕│р╕Щр╕зр╕У Gradient
$$
\frac{\partial L}{\partial b_0} = -2 \sum (y_i - b_0 - b_1x_i) = -2(20) = -40
$$
$$
\frac{\partial L}{\partial b_1} = -2 \sum x_i(y_i - b_0 - b_1x_i) = -2(66) = -132
$$

р╕Др╣Ир╕▓р╣Ар╕Йр╕ер╕╡р╣Ир╕в (р╕лр╕▓р╕гр╕Фр╣Йр╕зр╕в n = 5):
$$
\nabla b_0 = -8, \quad \nabla b_1 = -26.4
$$

---

### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 3: р╕Ыр╕гр╕▒р╕Ър╕Др╣Ир╕▓р╕Юр╕▓р╕гр╕▓р╕бр╕┤р╣Ар╕Хр╕нр╕гр╣М
$$
b_0^{(1)} = b_0^{(0)} - \alpha \nabla b_0 = 0 - 0.05(-8) = 0.4
$$
$$
b_1^{(1)} = b_1^{(0)} - \alpha \nabla b_1 = 0 - 0.05(-26.4) = 1.32
$$

---

### р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 4: р╕кр╕бр╕Бр╕▓р╕гр╕лр╕ер╕▒р╕Зр╕гр╕нр╕Ър╣Бр╕гр╕Б
$$
\hat{y} = 0.4 + 1.32x
$$
р╣Бр╕ер╕░р╣Ар╕бр╕╖р╣Ир╕нр╕Чр╕│р╕Лр╣Йр╕│р╕лр╕ер╕▓р╕вр╕гр╕нр╕Ъ (iterations)  
р╕Др╣Ир╕▓ SSE р╕Ир╕░р╕Др╣Ир╕нр╕в р╣Ж р╕ер╕Фр╕ер╕Зр╕Ир╕Щр╕Цр╕╢р╕Зр╕Др╣Ир╕▓р╕Хр╣Ир╕│р╕кр╕╕р╕Ф

---

## ЁЯУШ р╕кр╕гр╕╕р╕Ыр╕Бр╕▓р╕гр╣Ар╕Ыр╕гр╕╡р╕вр╕Ър╣Ар╕Чр╕╡р╕вр╕Ъ

| р╕зр╕┤р╕Шр╕╡ | р╣Бр╕Щр╕зр╕Др╕┤р╕Фр╕лр╕ер╕▒р╕Б | р╕Ир╕╕р╕Фр╣Ар╕Фр╣Ир╕Щ | р╕Ир╕╕р╕Фр╕Фр╣Йр╕нр╕в |
|:--|:--|:--|:--|
| **Least Squares** | р╕Др╕│р╕Щр╕зр╕Ур╕Др╣Ир╕▓р╣Ар╕лр╕бр╕▓р╕░р╕кр╕бр╣Вр╕Фр╕вр╕Хр╕гр╕З (р╕кр╕╣р╕Хр╕гр╕Ыр╕┤р╕Ф) | р╣Др╕Фр╣Йр╕Др╕│р╕Хр╕нр╕Ър╣Ар╕гр╣Зр╕зр╣Бр╕ер╕░р╣Бр╕бр╣Ир╕Щ | р╣Гр╕Кр╣Йр╣Др╕Фр╣Йр╣Ар╕Йр╕Юр╕▓р╕░р╣Вр╕бр╣Ар╕Фр╕ер╣Ар╕Кр╕┤р╕Зр╣Ар╕кр╣Йр╕Щ |
| **Gradient Descent** | р╕Ыр╕гр╕▒р╕Ър╕Др╣Ир╕▓р╕Чр╕╡р╕ер╕░р╕Вр╕▒р╣Йр╕Щр╕Ир╕Щ Loss р╕Хр╣Ир╕│р╕кр╕╕р╕Ф | р╣Гр╕Кр╣Йр╣Др╕Фр╣Йр╕Бр╕▒р╕Ър╣Вр╕бр╣Ар╕Фр╕ер╕Лр╕▒р╕Ър╕Лр╣Йр╕нр╕Щ р╣Ар╕Кр╣Ир╕Щ Neural Network | р╣Гр╕Кр╣Йр╣Ар╕зр╕ер╕▓р╕Щр╕▓р╕Щ р╕Хр╣Йр╕нр╕Зр╕Бр╕│р╕лр╕Щр╕Ф Learning rate р╕Чр╕╡р╣Ир╣Ар╕лр╕бр╕▓р╕░р╕кр╕б |

---

> ЁЯТм *тАЬр╕кр╕╣р╕Хр╕гр╕Др╕╖р╕нр╣Бр╕Ьр╕Щр╕Чр╕╡р╣И р╣Бр╕Хр╣Ир╕Бр╕▓р╕гр╕Др╕│р╕Щр╕зр╕Ур╕Чр╕╡р╕ер╕░р╕Вр╕▒р╣Йр╕Щр╕Др╕╖р╕нр╣Ар╕кр╣Йр╕Щр╕Чр╕▓р╕Зр╕кр╕╣р╣Ир╕Др╕зр╕▓р╕бр╣Ар╕Вр╣Йр╕▓р╣Гр╕Ир╕Чр╕╡р╣Ир╣Бр╕Чр╣Йр╕Ир╕гр╕┤р╕ЗтАЭ*
