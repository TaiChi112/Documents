# üßÆ ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 8 ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Optimization)

---

## üìò ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Key Terms)

| ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|:--|:--|
| **Optimization** | ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÇ‡∏î‡∏¢‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (Loss) ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î |
| **Loss Function** | ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô SSE, MSE |
| **Residual** | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á:  $e_i = y_i - \hat{y_i}$ |
| **SSE (Sum of Squared Errors)** | ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏¢‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏ß‡∏° |
| **R¬≤ (Coefficient of Determination)** | ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô (‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0‚Äì1) |
| **Gradient Descent** | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Loss ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡πà‡∏≤ Loss |
| **Learning Rate** | ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á |
| **Epoch** | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∏‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á |

---

## üìä 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î

### ‚úÖ Residual
$$
e_i = y_i - \hat{y_i}
$$

---

### ‚úÖ SSE (Sum of Squared Errors)
$$
SSE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2
$$

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**

| X | Y | ≈∂ | e | e¬≤ |
|:-:|:-:|:-:|:-:|:-:|
| 1 | 2 | 2.8 | -0.8 | 0.64 |
| 2 | 4 | 3.4 | 0.6 | 0.36 |
| 3 | 5 | 4.0 | 1.0 | 1.00 |
| 4 | 4 | 4.6 | -0.6 | 0.36 |
| 5 | 5 | 5.2 | -0.2 | 0.04 |

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:
$$
SSE = 2.40
$$

---

### ‚úÖ R¬≤ (Coefficient of Determination)
$$
R^2 = 1 - \frac{SSE}{SST}
$$

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
$$
SST = \sum_{i=1}^{n}(y_i - \bar{y})^2
$$

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**  
‡∏ñ‡πâ‡∏≤ $SSE = 2.4$ ‡πÅ‡∏•‡∏∞ $SST = 6$

$$
R^2 = 1 - \frac{2.4}{6} = 0.6
$$

---

## üìà 2. Least Squares Method (‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)

### ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ $\beta_0, \beta_1$ ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ SSE ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î

---

### ‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏™‡∏±‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå:

$$
b_1 = \frac{S_{xy}}{S_{xx}}, \quad b_0 = \bar{y} - b_1 \bar{x}
$$

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
$$
S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y}), \quad S_{xx} = \sum (x_i - \bar{x})^2
$$

---

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
| X | Y |
|:-:|:-:|
| 1 | 2 |
| 2 | 4 |
| 3 | 5 |
| 4 | 4 |
| 5 | 5 |

‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢:
$$
\bar{x} = 3, \quad \bar{y} = 4
$$

‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤:
$$
b_1 = 0.6, \quad b_0 = 2.2
$$

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡∏Ñ‡∏∑‡∏≠:
$$
\hat{y} = 2.2 + 0.6x
$$

---

## ‚öôÔ∏è 3. Gradient Descent (‡∏ß‡∏¥‡∏ò‡∏µ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô)

### ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:
$$
\theta := \theta - \alpha \frac{\partial L}{\partial \theta}
$$

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
- $\alpha$ = Learning rate  
- $\frac{\partial L}{\partial \theta}$ = ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Loss

---

### ‡πÉ‡∏ô Linear Regression:
$$
SSE(b_0,b_1) = \sum_{i=1}^{n} (y_i - (b_0 + b_1 x_i))^2
$$

‡∏´‡∏≤‡∏≠‡∏ô‡∏∏‡∏û‡∏±‡∏ô‡∏ò‡πå:
$$
\frac{\partial L}{\partial b_0} = -2 \sum (y_i - b_0 - b_1 x_i)
$$
$$
\frac{\partial L}{\partial b_1} = -2 \sum x_i (y_i - b_0 - b_1 x_i)
$$

‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:
$$
b_0^{(t+1)} = b_0^{(t)} - \alpha \frac{\partial L}{\partial b_0}
$$
$$
b_1^{(t+1)} = b_1^{(t)} - \alpha \frac{\partial L}{\partial b_1}
$$

---

### üîÅ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì (Iteration 1)
‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô:
$$
b_0^{(0)} = 0, \quad b_1^{(0)} = 0, \quad \alpha = 0.05
$$

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
$$
(X,Y) = \{(1,2),(2,4),(3,5),(4,4),(5,5)\}
$$

1Ô∏è‚É£ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì residual:
$$
e_i = y_i - (b_0 + b_1 x_i) = [2,4,5,4,5]
$$

2Ô∏è‚É£ Gradient:
$$
\frac{\partial L}{\partial b_0} = -\frac{2}{5}(20) = -8
$$
$$
\frac{\partial L}{\partial b_1} = -\frac{2}{5}(66) = -26.4
$$

3Ô∏è‚É£ ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤:
$$
b_0^{(1)} = 0 - 0.05(-8) = 0.4
$$
$$
b_1^{(1)} = 0 - 0.05(-26.4) = 1.32
$$

‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å:
$$
\hat{y} = 0.4 + 1.32x
$$

---

## üìò ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö

| ‡∏ß‡∏¥‡∏ò‡∏µ | ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô | ‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢ |
|:--|:--|:--|:--|
| **Least Squares** | ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ SSE ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏¥‡∏î | ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô |
| **Gradient Descent** | ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏à‡∏ô Loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Learning rate ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° |

---

> üí¨ *"Optimization ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Machine Learning ‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏•‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô"*
