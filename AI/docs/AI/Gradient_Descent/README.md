# üìò ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 8 ‚Äî ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Optimization)

---

## üß© ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Key Terms)

| ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå                                  | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢                                                                                     |
| :------------------------------------ | :----------------------------------------------------------------------------------------- |
| **Optimization (‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û)**    | ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (Error) ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢       |
| **Loss Function (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)**   | ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á ($y_i$) ‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ($\hat{y}_i$)                       |
| **Residual**                          | ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ $\Rightarrow e_i = y_i - \hat{y}_i$                             |
| **SSE (Sum of Squared Errors)**       | ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á residuals ‡∏¢‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•                                     |
| **R¬≤ (Coefficient of Determination)** | ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÉ‡∏î ($0 ‚â§ R¬≤ ‚â§ 1$)                             |
| **Gradient Descent (‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô)**    | ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î |
| **Learning Rate (Œ±)**                 | ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏∞ ‚Äú‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏°‚Äù ‡∏à‡∏∏‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏ñ‡πâ‡∏≤‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤           |

---

## ‚öôÔ∏è ‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

### 1. Residual
$$
e_i = y_i - \hat{y}_i
$$

---

### 2. Sum of Squared Errors (SSE)
$$
SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

---

### 3. R¬≤ (Coefficient of Determination)
$$
R^2 = 1 - \frac{SSE}{SST}
$$
‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
$$
SST = \sum_{i=1}^{n} (y_i - \bar{y})^2
$$

---

### 4. ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡πâ‡∏ô‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (Linear Regression)
$$
\hat{y}_i = b_0 + b_1x_i
$$

---

### 5. ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡πÅ‡∏Å‡∏ô Y
$$
b_1 = \frac{S_{xy}}{S_{xx}}, \quad b_0 = \bar{y} - b_1\bar{x}
$$
‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
$$
S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y}), \quad S_{xx} = \sum (x_i - \bar{x})^2
$$

---

### 6. Gradient Descent
$$
\theta \leftarrow \theta - \alpha \nabla f(\theta)
$$

‡∏Å‡∏£‡∏ì‡∏µ Linear Regression:
$$
b_0^{(t+1)} = b_0^{(t)} - \alpha \frac{\partial SSE}{\partial b_0}
$$
$$
b_1^{(t+1)} = b_1^{(t)} - \alpha \frac{\partial SSE}{\partial b_1}
$$

---

## üßÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà 1 ‚Äî Least Squares Method

**‡πÇ‡∏à‡∏ó‡∏¢‡πå:**  
‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
|   X   |   Y   |
| :---: | :---: |
|   1   |   2   |
|   2   |   4   |
|   3   |   5   |
|   4   |   4   |
|   5   |   5   |

‡∏à‡∏á‡∏´‡∏≤‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡πâ‡∏ô‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô $\hat{y} = b_0 + b_1x$

---

### ‚úÖ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì

1. **‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢**
$$
\bar{x} = 3, \quad \bar{y} = 4
$$

2. **‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì $S_{xy}$ ‡πÅ‡∏•‡∏∞ $S_{xx}$**

| X   | Y   | (X‚àíxÃÑ) | (Y‚àí»≥) | (X‚àíxÃÑ)(Y‚àí»≥) | (X‚àíxÃÑ)¬≤ |
| --- | --- | ----- | ----- | ---------- | ------ |
| 1   | 2   | ‚àí2    | ‚àí2    | 4          | 4      |
| 2   | 4   | ‚àí1    | 0     | 0          | 1      |
| 3   | 5   | 0     | 1     | 0          | 0      |
| 4   | 4   | 1     | 0     | 0          | 1      |
| 5   | 5   | 2     | 1     | 2          | 4      |

$$
S_{xy} = 6, \quad S_{xx} = 10
$$

3. **‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î**

$$
b_1 = \frac{6}{10} = 0.6, \quad b_0 = 4 - (0.6√ó3) = 2.2
$$

4. **‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡πâ‡∏ô‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢**

$$
\hat{y} = 2.2 + 0.6x
$$

---

### ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•

|   X   |   Y   |   ≈∂   | Residual | Residual¬≤ |
| :---: | :---: | :---: | :------: | :-------: |
|   1   |   2   |  2.8  |   ‚àí0.8   |   0.64    |
|   2   |   4   |  3.4  |   0.6    |   0.36    |
|   3   |   5   |  4.0  |   1.0    |   1.00    |
|   4   |   4   |  4.6  |   ‚àí0.6   |   0.36    |
|   5   |   5   |  5.2  |   ‚àí0.2   |   0.04    |

$$
SSE = 2.40, \quad SST = 6.00, \quad R^2 = 1 - \frac{2.4}{6} = 0.6
$$

**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:**  
‡∏™‡∏°‡∏Å‡∏≤‡∏£ $\hat{y} = 2.2 + 0.6x$ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ 60%

---

## üßÆ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà 2 ‚Äî Gradient Descent

**‡πÇ‡∏à‡∏ó‡∏¢‡πå:**  
‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (X,Y)  
‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ $b_0=0$, $b_1=0$, ‡∏Ñ‡πà‡∏≤ learning rate = 0.05  
‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå 1 ‡∏£‡∏≠‡∏ö (iteration)

---

### ‚úÖ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

1. **‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ (SSE):**
$$
SSE(b_0,b_1) = \sum (y_i - (b_0 + b_1x_i))^2
$$

2. **‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Gradient:**
$$
\frac{\partial SSE}{\partial b_0} = -2\sum (y_i - b_0 - b_1x_i)
$$
$$
\frac{\partial SSE}{\partial b_1} = -2\sum x_i(y_i - b_0 - b_1x_i)
$$

3. **‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: $b_0^{(0)}=0$, $b_1^{(0)}=0$**
$$
\sum y_i = 20, \quad \sum x_iy_i = 66
$$
‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô
$$
\nabla b_0 = -\frac{2}{5}(20) = -8, \quad \nabla b_1 = -\frac{2}{5}(66) = -26.4
$$

4. **‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå**
$$
b_0^{(1)} = 0 - 0.05(-8) = 0.4
$$
$$
b_1^{(1)} = 0 - 0.05(-26.4) = 1.32
$$

5. **‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 1**
$$
\hat{y} = 0.4 + 1.32x
$$

---

### ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì SSE ‡πÉ‡∏´‡∏°‡πà
|   X   |   Y   |   ≈∂   | Residual | Residual¬≤ |
| :---: | :---: | :---: | :------: | :-------: |
|   1   |   2   | 1.72  |   0.28   |  0.0784   |
|   2   |   4   | 3.04  |   0.96   |  0.9216   |
|   3   |   5   | 4.36  |   0.64   |  0.4096   |
|   4   |   4   | 5.68  |  ‚àí1.68   |  2.8224   |
|   5   |   5   | 7.00  |  ‚àí2.00   |  4.0000   |

$$
SSE = 8.2320
$$

---

### üîÅ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏£‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡πÑ‡∏õ (Iteration 2‚Äì5)

‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì $\nabla b_0, \nabla b_1$ ‡πÉ‡∏´‡∏°‡πà  
‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï $b_0, b_1$ ‡∏ï‡∏≤‡∏°‡∏™‡∏°‡∏Å‡∏≤‡∏£  
‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á SSE ‡∏à‡∏∞ **‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢ ‡πÜ** ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏∏‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

---

## üìä ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ

| ‡∏ß‡∏¥‡∏ò‡∏µ                   | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô                                        | ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î                                                 |
| :------------------- | :------------------------------------------- | :---------------------------------------------------- |
| **Least Squares**    | ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏à‡∏∏‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (Closed-form Solution) | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô                           |
| **Gradient Descent** | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô Neural Network          | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Learning Rate ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö (Epochs) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° |

---

## üí° ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î

> ‚ÄúLeast Squares ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥  
> Gradient Descent ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‚Äù

---

## üß† ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£)

1. **‡πÇ‡∏à‡∏ó‡∏¢‡πå 1:**  
   ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 6 ‡∏à‡∏∏‡∏î $(1,2)$, $(2,3)$, $(3,4)$, $(4,5)$, $(5,6)$, $(6,7)$  
   - ‡∏à‡∏á‡∏´‡∏≤‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  
   - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì $R^2$

2. **‡πÇ‡∏à‡∏ó‡∏¢‡πå 2:**  
   ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ $b_0=0$, $b_1=0$, $\alpha=0.1$  
   ‡∏ó‡∏≥ Gradient Descent 2 ‡∏£‡∏≠‡∏ö ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

---

> üí¨ ‚Äú‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Optimization ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (Machine Learning) ‚Äî ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î Error ‡πÄ‡∏™‡∏°‡∏≠‚Äù
