# üßÆ ‡∏ö‡∏ó‡∏ó‡∏µ‡πà 8 ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Optimization)

---

## üìò ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Key Terms)

| ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|:--|:--|
| **Optimization** | ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÇ‡∏î‡∏¢‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (Loss) ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î |
| **Loss Function** | ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô SSE, MSE |
| **Residual** | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á:  $e_i = y_i - \hat{y_i}$ |
| **SSE (Sum of Squared Errors)** | ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏¢‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏ß‡∏° |
| **R¬≤ (Coefficient of Determination)** | ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô (‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0‚Äì1) |
| **Gradient Descent** | ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Loss ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡πà‡∏≤ Loss |
| **Learning Rate** | ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á |
| **Epoch** | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∏‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á |

---

## üìä ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏π‡∏ï‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

### ‚úÖ Residual
$$
e_i = y_i - \hat{y_i}
$$

### ‚úÖ SSE (Sum of Squared Errors)
$$
SSE = \sum_{i=1}^{n} (y_i - \hat{y_i})^2
$$

### ‚úÖ R¬≤ (Coefficient of Determination)
$$
R^2 = 1 - \frac{SSE}{SST}
$$
‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
$$
SST = \sum_{i=1}^{n}(y_i - \bar{y})^2
$$

---

### ‚úÖ ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (Linear Regression)
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ $\beta_0, \beta_1$ ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ SSE ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î

$$
b_1 = \frac{S_{xy}}{S_{xx}}, \quad b_0 = \bar{y} - b_1 \bar{x}
$$
‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
$$
S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y}), \quad S_{xx} = \sum (x_i - \bar{x})^2
$$

---

### ‚úÖ Gradient Descent
$$
\theta := \theta - \alpha \frac{\partial L}{\partial \theta}
$$

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà  
- $\alpha$ = Learning rate  
- $\frac{\partial L}{\partial \theta}$ = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á Loss

‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ Linear Regression:
$$
\frac{\partial L}{\partial b_0} = -2 \sum (y_i - b_0 - b_1 x_i)
$$
$$
\frac{\partial L}{\partial b_1} = -2 \sum x_i (y_i - b_0 - b_1 x_i)
$$

---

## üß† ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏π‡∏ï‡∏£

---

### üß© **‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡∏î‡πâ‡∏ß‡∏¢ Least Squares**

‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

| X | Y |
|:-:|:-:|
| 1 | 2 |
| 2 | 4 |
| 3 | 5 |
| 4 | 4 |
| 5 | 5 |

1. ‡∏à‡∏á‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ $\bar{x}$ ‡πÅ‡∏•‡∏∞ $\bar{y}$  
2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ $b_1$ ‡πÅ‡∏•‡∏∞ $b_0$ ‡∏à‡∏≤‡∏Å‡∏™‡∏π‡∏ï‡∏£ Least Squares  
3. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ

**‡πÄ‡∏â‡∏•‡∏¢‡∏¢‡πà‡∏≠:**
$$
\bar{x} = 3, \quad \bar{y} = 4
$$
$$
S_{xx} = 10, \quad S_{xy} = 6
$$
$$
b_1 = 0.6, \quad b_0 = 2.2
$$
‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠:
$$
\hat{y} = 2.2 + 0.6x
$$

---

### ‚öôÔ∏è **‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà 2: ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ Gradient Descent**

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:  
$(X,Y) = \{(1,2),(2,4),(3,5),(4,4),(5,5)\}$  
‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô:  
$b_0^{(0)} = 0, \quad b_1^{(0)} = 0, \quad \alpha = 0.05$

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**
1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô (gradient) ‡∏Ç‡∏≠‡∏á $b_0$ ‡πÅ‡∏•‡∏∞ $b_1$  
2. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤ $b_0$ ‡πÅ‡∏•‡∏∞ $b_1$ ‡∏´‡∏•‡∏±‡∏á‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å (iteration 1)

**‡πÄ‡∏â‡∏•‡∏¢‡∏¢‡πà‡∏≠:**

$$
\frac{\partial L}{\partial b_0} = -8, \quad \frac{\partial L}{\partial b_1} = -26.4
$$

‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï:
$$
b_0^{(1)} = 0.4, \quad b_1^{(1)} = 1.32
$$

‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏á‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å:
$$
\hat{y} = 0.4 + 1.32x
$$

---

### üí° ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ö‡∏ó

| ‡∏ß‡∏¥‡∏ò‡∏µ | ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô | ‡∏à‡∏∏‡∏î‡∏î‡πâ‡∏≠‡∏¢ |
|:--|:--|:--|:--|
| **Least Squares** | ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ SSE ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á | ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô |
| **Gradient Descent** | ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏à‡∏ô‡∏Ñ‡πà‡∏≤ Loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î | ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô | ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Learning rate ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° |

---

> üí¨ *"‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á"*
