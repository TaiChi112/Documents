# 🧠 บทที่ 10 — การเรียนรู้เชิงลึก (Deep Learning)

---

## 📘 คำศัพท์สำคัญ (Key Terms)

| คำศัพท์                                   | คำอธิบาย                                                                                            |
| :------------------------------------- | :------------------------------------------------------------------------------------------------ |
| **Deep Learning (การเรียนรู้เชิงลึก)**      | กระบวนการเรียนรู้ของเครื่องที่ใช้โครงข่ายประสาทเทียมหลายชั้น (ANN หลายชั้น) เพื่อเรียนรู้คุณลักษณะที่ซับซ้อนจากข้อมูลจำนวนมาก |
| **Neural Network (โครงข่ายประสาทเทียม)** | โครงสร้างที่ประกอบด้วยชั้นอินพุต (input layer), ชั้นซ่อน (hidden layer) และชั้นเอาต์พุต (output layer)           |
| **Convolutional Neural Network (CNN)** | โครงข่ายประสาทเทียมที่ออกแบบมาเฉพาะสำหรับข้อมูลเชิงพื้นที่ เช่น ภาพและวิดีโอ โดยใช้การคอนโวลูชัน (convolution)       |
| **Filter / Kernel (ตัวกรอง)**           | เมทริกซ์เล็ก ๆ ที่ใช้สไลด์ไปบนภาพอินพุตเพื่อสกัดคุณลักษณะ (feature extraction)                                  |
| **Stride (การก้าว)**                    | จำนวนพิกเซลที่ตัวกรองเลื่อนในแต่ละครั้ง                                                                     |
| **Padding**                            | การเติมขอบรอบภาพ เพื่อคงขนาดเอาต์พุตให้เท่ากับอินพุต                                                        |
| **Pooling (การย่อลง)**                  | การลดขนาดภาพ เช่น Max Pooling, Average Pooling เพื่อลดจำนวนพารามิเตอร์                                  |
| **Activation Function (ฟังก์ชันกระตุ้น)**   | ฟังก์ชันที่เพิ่มความไม่เชิงเส้นให้โมเดล เช่น ReLU, tanh, Softmax                                              |
| **Feature Map**                        | ผลลัพธ์จากการทำคอนโวลูชัน                                                                              |
| **Flattening**                         | การแปลงข้อมูลภาพหลายมิติให้เป็นเวกเตอร์หนึ่งมิติก่อนเข้าสู่ Fully Connected Layer                                |
| **Fully Connected Layer (FC Layer)**   | ชั้นที่เชื่อมทุกนิวรอนของชั้นก่อนหน้าเข้ากับทุกนิวรอนของชั้นถัดไป เพื่อสรุปผลการจำแนก                                    |

---

## ⚙️ ส่วนที่ 1 : สมการหลักของ CNN

### ✅ สมการคอนโวลูชัน 2 มิติ

$$
S(i, j) = (I * K)(i, j) = \sum_{m=0}^{M-1}\sum_{n=0}^{N-1} I(i+m, j+n) \cdot K(m, n) + b
$$

| สัญลักษณ์ | ความหมาย           |
| :----- | :----------------- |
| $I$    | อินพุต (Input Image) |
| $K$    | ตัวกรองหรือ Kernel   |
| $b$    | ค่าไบแอส            |
| $M, N$ | ขนาดของ Kernel     |

---

### ✅ สมการหาขนาดเอาต์พุตของชั้นคอนโวลูชัน

$$
W_{out} = \frac{W - F + 2p}{s} + 1
$$

$$
H_{out} = \frac{H - F + 2p}{s} + 1
$$

| สัญลักษณ์             | ความหมาย                 |
| :----------------- | :----------------------- |
| $W, H$             | ความกว้างและความสูงของอินพุต |
| $F$                | ขนาดของฟิลเตอร์            |
| $p$                | จำนวน padding             |
| $s$                | stride                   |
| $W_{out}, H_{out}$ | ขนาดเอาต์พุต               |

---

## ⚙️ ส่วนที่ 2 : ฟังก์ชันกระตุ้น (Activation Functions)

### ✅ ReLU (Rectified Linear Unit)

$$
f(x) = \max(0, x)
$$

- ถ้า $x > 0$ → ได้ค่า $x$  
- ถ้า $x < 0$ → ได้ค่า 0  
ช่วยลดปัญหา *vanishing gradient* และเพิ่มความเร็วในการเรียนรู้

---

### ✅ tanh (Hyperbolic Tangent)

$$
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

- ช่วงของค่าอยู่ใน [-1, 1]  
- ศูนย์กลางอยู่ที่ 0 → เหมาะกับข้อมูลที่มีทั้งบวกและลบ

---

### ✅ Softmax (สำหรับการจำแนกหลายคลาส)

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

- ใช้ในชั้นเอาต์พุต (Output Layer)
- ให้ผลลัพธ์เป็นความน่าจะเป็นรวม = 1

---

## 🧮 ส่วนที่ 3 : การย่อลง (Pooling Layer)

### ✅ Max Pooling
เลือกค่ามากที่สุดในพื้นที่ย่อย เช่น 2×2

### ✅ Average Pooling
เฉลี่ยค่าภายในพื้นที่ย่อย

---

## 📘 ตัวอย่างโจทย์ที่ 1 — การคำนวณ Output ของ Convolution Layer

**กำหนดให้:**
- ขนาดอินพุต (Input) = 8×8  
- ขนาดฟิลเตอร์ (Filter) = 3×3  
- Stride = 1  
- Padding = 1  

**จงหาขนาดของ Output Feature Map**

---

### ✅ วิธีแก้แบบละเอียด

ใช้สูตร:

$$
W_{out} = \frac{W - F + 2p}{s} + 1
$$

แทนค่า:

$$
W_{out} = \frac{8 - 3 + 2(1)}{1} + 1 = 8
$$

ดังนั้น

$$
Output = 8×8
$$

**คำตอบ:** ขนาดเอาต์พุตเท่ากับ **8×8**

---

## 📘 ตัวอย่างโจทย์ที่ 2 — การทำ Convolution เชิงตัวเลข

**Input Image (3×3):**

$$
\begin{bmatrix}
1 & 2 & 0 \\
0 & 1 & 3 \\
2 & 1 & 1
\end{bmatrix}
$$

**Kernel (Filter 2×2):**

$$  
\begin{bmatrix}
1 & 0 \\
-1 & 1
\end{bmatrix}
$$

**Stride = 1, Padding = 0**

---

### ✅ ขั้นตอนที่ 1: วาง Kernel บนภาพ

**จุดแรก (บนซ้าย):**

$$
(1×1) + (2×0) + (0×-1) + (1×1) = 2
$$

**จุดถัดไป (บนขวา):**

$$
(2×1) + (0×0) + (1×-1) + (3×1) = 4
$$

ทำเช่นนี้ทุกตำแหน่ง → จะได้ผลลัพธ์ 2×2

---

**Output Feature Map:**

$$
\begin{bmatrix}
2 & 4 \\
-1 & 3
\end{bmatrix}
$$

---

## 📘 ตัวอย่างโจทย์ที่ 3 — Max Pooling

**Input (Feature Map):**

$$  
\begin{bmatrix}
2 & 4 \\
-1 & 3
\end{bmatrix}
$$

**Filter 2×2, Stride = 2**

---

**ขั้นตอน:**
เลือกค่ามากสุดจากทุกช่องในขนาด 2×2 →  

$$
\max(2, 4, -1, 3) = 4
$$

**Output = [4]**

---

## 📊 ส่วนที่ 4 : การเปรียบเทียบฟังก์ชันกระตุ้น

| ฟังก์ชัน       | สมการ                                      | ช่วงค่า  | จุดเด่น                      | จุดด้อย                |
| :---------- | :----------------------------------------- | :----- | :------------------------- | :------------------- |
| **ReLU**    | $f(x)=\max(0,x)$                           | [0,∞)  | เร็ว, ลด vanishing gradient | ไม่เรียนรู้เมื่อค่า < 0     |
| **tanh**    | $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$   | [-1,1] | สมมาตรรอบศูนย์               | gradient หายถ้า       | x | ใหญ่ |
| **Softmax** | $\sigma(z_i)=\frac{e^{z_i}}{\sum e^{z_j}}$ | [0,1]  | แปลงผลลัพธ์เป็นความน่าจะเป็น    | คำนวณหนักในคลาสจำนวนมาก |

---

## 🧠 สรุปสูตรที่ควรรู้

| สูตร                                        | ชื่อ          | การใช้งาน             |
| :----------------------------------------- | :---------- | :------------------- |
| $S(i,j)=\sum I(i+m,j+n)K(m,n)+b$           | Convolution | ใช้สกัดคุณลักษณะจากภาพ   |
| $W_{out}=\frac{W-F+2p}{s}+1$               | Output Size | ใช้หาขนาดของผลลัพธ์     |
| $f(x)=\max(0,x)$                           | ReLU        | เพิ่มความไม่เชิงเส้น      |
| $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$   | tanh        | ใช้กับข้อมูลบวก-ลบ       |
| $\sigma(z_i)=\frac{e^{z_i}}{\sum e^{z_j}}$ | Softmax     | แปลงค่าเป็นความน่าจะเป็น |
| **Pooling:** $\max$ หรือ $\text{avg}$       | ลดขนาดข้อมูล  |

---

> 💬 *“Deep Learning ไม่ใช่แค่เรื่องของชั้นที่มากขึ้น แต่คือการเข้าใจลึกขึ้นว่าข้อมูลซ่อนอะไรอยู่”*
