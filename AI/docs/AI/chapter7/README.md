# üìß Naive Bayes Spam Classification Project

## üéØ Overview
‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ **Naive Bayes Algorithm** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Spam ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏ö‡∏ö step-by-step

---

## üìÅ Project Structure

```
chapter7/
‚îú‚îÄ‚îÄ Naive_Bayes_classification (1).ipynb    # ‡∏´‡∏•‡∏±‡∏Å: ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ SMS Dataset ‡∏à‡∏£‡∏¥‡∏á
‚îú‚îÄ‚îÄ naive_bayes_assignment.ipynb           # ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
‚îú‚îÄ‚îÄ message_probability_assignment.ipynb   # ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì specific message
‚îú‚îÄ‚îÄ smsspamcollection/                     # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SMS dataset
‚îî‚îÄ‚îÄ README.md                              # ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ
```

---

## üìö Files Description

### 1. üìä `Naive_Bayes_classification (1).ipynb`
**‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å Spam ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SMS ‡∏à‡∏£‡∏¥‡∏á + ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô Train/Test**

#### üéØ **‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:**
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SMS ‡∏à‡∏£‡∏¥‡∏á (5,572 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°) ‡∏à‡∏≤‡∏Å UCI ML Repository
- ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test (60%, 80%, 95%)
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•

#### üîç **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°:**
- **Data Preprocessing**: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, tokenization, lowercase conversion
- **Vocabulary Building**: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å
- **Model Training**: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Prior ‡πÅ‡∏•‡∏∞ Conditional Probabilities
- **Experimental Design**: ‡∏ó‡∏î‡∏•‡∏≠‡∏á 3 ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- **Performance Analysis**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Accuracy, Precision, Recall, F1-Score
- **Visualization**: ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö

#### üìà **‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á:**
| Train % | Test Size | Vocabulary | Accuracy | Precision | Recall | F1-Score |
|---------|-----------|------------|----------|-----------|--------|----------|
| 60%     | 2,229     | 9,996      | 99.01%   | 0.9828    | 0.9468 | 0.9645   |
| 80%     | 1,114     | 11,860     | 98.92%   | 0.9653    | 0.9586 | 0.9619   |
| 95%     | 279       | 13,147     | 99.64%   | 1.0000    | 0.9688 | 0.9841   |

#### üéØ **‡∏Ç‡πâ‡∏≠‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö:**
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí Vocabulary ‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí Coverage ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
- ‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (95% train) ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠
- 80% train ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠

---

### 2. üéì `naive_bayes_assignment.ipynb`
**‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Naive Bayes ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢ ‡πÜ**

#### üéØ **‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:**
- ‡∏™‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ Naive Bayes ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
- ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

#### üìä **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:**
```
| ‡∏Ñ‡∏≥   | Spam | No Spam |
|------|------|---------|
| Buy  | 20   | 5       |
| Cheap| 15   | 10      |
| Work | 5    | 30      |
| Free | 20   | 7       |
| Will | 4    | 40      |
Total  | 25   | 75      |
```

#### üîç **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°:**
- **Prior Probability**: P(Spam) = 1/4, P(No Spam) = 3/4
- **Conditional Probability**: P(word|class) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥
- **Interactive Classifier**: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
- **Word Analysis**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° spam/no spam
- **Multiple Word Testing**: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö message ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥
- **Laplace Smoothing**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô
- **Visualization**: ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á probability distributions

#### üßÆ **‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:**
- **Spam Words**: Buy (85.2%), Free (80.4%), Cheap (68.3%)
- **No Spam Words**: Will (12.6%), Work (19.3%)

#### üìà **Features:**
- Step-by-step calculation explanation
- Interactive probability calculator
- Comprehensive visualization
- Laplace smoothing demonstration

---

### 3. üßÆ `message_probability_assignment.ipynb`
**‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Probability ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Message ‡πÄ‡∏â‡∏û‡∏≤‡∏∞: "Buy, Cheap, Work & Free"**

#### üéØ **‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:**
- ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏ö‡∏ö manual step-by-step
- ‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö message ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
- ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î

#### üìä **‡πÇ‡∏à‡∏ó‡∏¢‡πå:**
‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì P(Spam|"Buy, Cheap, Work, Free") ‡πÅ‡∏•‡∏∞ P(No Spam|"Buy, Cheap, Work, Free")

#### üî¢ **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:**

1. **Prior Probabilities:**
   ```
   P(Spam) = 25/100 = 0.25
   P(No Spam) = 75/100 = 0.75
   ```

2. **Conditional Probabilities:**
   ```
   P(Buy|Spam) = 20/60 = 0.3333    P(Buy|No Spam) = 5/52 = 0.0962
   P(Cheap|Spam) = 15/60 = 0.2500  P(Cheap|No Spam) = 10/52 = 0.1923
   P(Work|Spam) = 5/60 = 0.0833    P(Work|No Spam) = 30/52 = 0.5769
   P(Free|Spam) = 20/60 = 0.3333   P(Free|No Spam) = 7/52 = 0.1346
   ```

3. **Likelihood Calculation:**
   ```
   P(Spam|Message) ‚àù 0.25 √ó 0.3333 √ó 0.2500 √ó 0.0833 √ó 0.3333 = 0.000578703704
   P(No Spam|Message) ‚àù 0.75 √ó 0.0962 √ó 0.1923 √ó 0.5769 √ó 0.1346 = 0.001077053281
   ```

4. **Final Results:**
   ```
   P(Spam|Message) = 34.95%
   P(No Spam|Message) = 65.05%
   ‚Üí Prediction: No Spam
   ```

#### üéØ **‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:**
Message **"Buy, Cheap, Work & Free"** ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô **65.05%** ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô **No Spam**

---

## üöÄ How to Use

### Prerequisites
```bash
pip install pandas numpy matplotlib seaborn jupyter
```

### Running the Notebooks

1. **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:** `naive_bayes_assignment.ipynb`
   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
   - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

2. **‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:** `message_probability_assignment.ipynb`
   - ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì manual
   - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

3. **‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á:** `Naive_Bayes_classification (1).ipynb`
   - ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
   - ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå

---

## üìö Learning Objectives

### üéØ **‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏ö ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:**
1. **‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ Naive Bayes** - ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô independence ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
2. **‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì probability** - Prior, Conditional, Posterior ‡πÅ‡∏ö‡∏ö step-by-step
3. **‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á** - Preprocessing, tokenization, feature extraction
4. **‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå** - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
5. **‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô** - ‡∏™‡∏£‡πâ‡∏≤‡∏á spam filter ‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏à‡∏£‡∏¥‡∏á

---

## üßÆ Mathematical Foundation

### Naive Bayes Formula:
```
P(Class|Message) = P(Class) √ó ‚àèP(word|Class)
```

### Key Assumptions:
1. **Independence**: ‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô message ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô
2. **Bag of Words**: ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
3. **Laplace Smoothing**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô training data

---

## üìä Performance Comparison

| Metric | Simple Example | Real SMS Data |
|--------|----------------|---------------|
| Dataset Size | 100 emails | 5,572 SMS |
| Vocabulary | 5 words | 11,860+ words |
| Classes | Balanced | Imbalanced (86% Ham) |
| Complexity | Educational | Production-ready |
| Use Case | Learning | Real Application |

---

## üéì Educational Value

### üí° **Progression Path:**
1. **Basic Theory** ‚Üí `naive_bayes_assignment.ipynb`
2. **Manual Calculation** ‚Üí `message_probability_assignment.ipynb`  
3. **Real Application** ‚Üí `Naive_Bayes_classification (1).ipynb`

### üîç **Key Insights:**
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚â† ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏™‡∏°‡∏≠‡πÑ‡∏õ
- Vocabulary size ‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠ model complexity
- Balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á training/testing ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å
- Laplace smoothing ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ zero probability

---

## üõ†Ô∏è Technical Implementation

### Data Processing Pipeline:
1. **Text Cleaning**: Remove punctuation, lowercase conversion
2. **Tokenization**: Split into individual words  
3. **Vocabulary Building**: Create word frequency tables
4. **Probability Calculation**: Prior and conditional probabilities
5. **Classification**: Apply Bayes rule with normalization

### Model Evaluation:
- **Accuracy**: Overall correctness
- **Precision**: True Positive / (True Positive + False Positive)
- **Recall**: True Positive / (True Positive + False Negative)
- **F1-Score**: Harmonic mean of Precision and Recall

---

## üéØ Next Steps

### üöÄ **Possible Improvements:**
1. **N-grams**: ‡πÉ‡∏ä‡πâ word pairs/triplets ‡πÅ‡∏ó‡∏ô single words
2. **TF-IDF**: Weight words by importance
3. **Feature Selection**: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ discriminative power ‡∏™‡∏π‡∏á
4. **Ensemble Methods**: ‡∏£‡∏ß‡∏° multiple classifiers
5. **Deep Learning**: ‡πÉ‡∏ä‡πâ Neural Networks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text classification

### üìà **Advanced Topics:**
- Multinomial vs Bernoulli Naive Bayes
- Handling imbalanced datasets
- Cross-validation strategies
- Production deployment considerations

---

## üìû Contact & Support

‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ï‡πà‡∏≤‡∏á ‡πÜ

---

**Happy Learning! üéâ**

*"The best way to learn machine learning is by doing it step by step."*