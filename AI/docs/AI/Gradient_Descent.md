# âš™ï¸ Gradient Descent  
> à¹à¸›à¸¥à¹à¸¥à¸°à¸‚à¸¢à¸²à¸¢à¸ˆà¸²à¸à¸ªà¹„à¸¥à¸”à¹Œ â€œGradient Descent â€” Artificial Intelligence (AI), COS3109â€

---

## ðŸŽ¯ à¸«à¸±à¸§à¸‚à¹‰à¸­ (Agenda)
- Linear Regression  
- Least Square Estimation  
- Gradient Descent  
- Workshop  

---

## ðŸ“ˆ Linear Regression (à¸à¸²à¸£à¸–à¸”à¸–à¸­à¸¢à¹€à¸Šà¸´à¸‡à¹€à¸ªà¹‰à¸™)

**à¹à¸™à¸§à¸„à¸´à¸”à¸«à¸¥à¸±à¸:**  
à¸à¸²à¸£à¸«à¸²à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸•à¸±à¸§à¹à¸›à¸£à¸­à¸´à¸ªà¸£à¸° (Independent variable) à¸à¸±à¸šà¸•à¸±à¸§à¹à¸›à¸£à¸•à¸²à¸¡ (Dependent variable)  

**à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™:**  
- à¸˜à¸¸à¸£à¸à¸´à¸ˆ (Business analytics)  
- à¸à¸²à¸£à¸•à¸¥à¸²à¸”à¸—à¸µà¹ˆà¸‚à¸±à¸šà¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¸”à¹‰à¸§à¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (Data-driven marketing)  
- à¸Šà¸µà¸§à¸§à¸´à¸—à¸¢à¸² (Biology)  
- à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™ Machine Learning  

---

### ðŸ§© à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸–à¸²à¸™à¸à¸²à¸£à¸“à¹Œ

à¸¨à¸¶à¸à¸©à¸²à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡  
> à¸¢à¸­à¸”à¸‚à¸²à¸¢à¸­à¸µà¸„à¸­à¸¡à¹€à¸¡à¸´à¸£à¹Œà¸‹à¸£à¸²à¸¢à¹€à¸”à¸·à¸­à¸™ (Y)  
> à¸à¸±à¸š à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¸”à¹‰à¸²à¸™à¸à¸²à¸£à¹‚à¸†à¸©à¸“à¸²à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ (X)

à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸­à¸‡à¸£à¹‰à¸²à¸™à¸„à¹‰à¸²à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ 7 à¹à¸«à¹ˆà¸‡à¸•à¸¥à¸­à¸”à¸›à¸µà¸—à¸µà¹ˆà¸œà¹ˆà¸²à¸™à¸¡à¸²  
> **à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢:** à¸«à¸² â€œà¸ªà¸¡à¸à¸²à¸£à¹€à¸ªà¹‰à¸™à¸•à¸£à¸‡â€ à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (best fit line)

---

## ðŸ“˜ à¸ªà¸¡à¸à¸²à¸£à¸–à¸”à¸–à¸­à¸¢à¹€à¸Šà¸´à¸‡à¹€à¸ªà¹‰à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¸‡à¹ˆà¸²à¸¢ (Simple Linear Regression)

à¸¡à¸µà¸•à¸±à¸§à¹à¸›à¸£à¹€à¸žà¸µà¸¢à¸‡ 1 à¸•à¸±à¸§:
- \( X \): à¸•à¸±à¸§à¹à¸›à¸£à¸­à¸´à¸ªà¸£à¸° (Independent / Predictor variable)  
- \( Y \): à¸•à¸±à¸§à¹à¸›à¸£à¸•à¸²à¸¡ (Dependent / Outcome variable)

à¸ªà¸¡à¸à¸²à¸£à¹€à¸Šà¸´à¸‡à¹€à¸ªà¹‰à¸™à¸žà¸·à¹‰à¸™à¸à¸²à¸™à¸„à¸·à¸­:
\[
Y = mX + b
\]

à¹‚à¸”à¸¢à¸—à¸µà¹ˆ  
- \( m \) = à¸„à¸§à¸²à¸¡à¸Šà¸±à¸™à¸‚à¸­à¸‡à¹€à¸ªà¹‰à¸™ (Slope)  
- \( b \) = à¸ˆà¸¸à¸”à¸•à¸±à¸”à¹à¸à¸™ Y (Intercept)

---

## ðŸ§® Least Square Estimation (à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸²à¸“à¸”à¹‰à¸§à¸¢à¸œà¸¥à¸£à¸§à¸¡à¸à¸³à¸¥à¸±à¸‡à¸ªà¸­à¸‡à¸™à¹‰à¸­à¸¢à¸—à¸µà¹ˆà¸ªà¸¸à¸”)

**à¹à¸™à¸§à¸„à¸´à¸”:**  
à¸«à¸²à¸„à¹ˆà¸² \( m \) à¹à¸¥à¸° \( b \) à¸—à¸µà¹ˆà¸—à¸³à¹ƒà¸«à¹‰à¸œà¸¥à¸£à¸§à¸¡à¸‚à¸­à¸‡à¸„à¸§à¸²à¸¡à¸„à¸¥à¸²à¸”à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸„à¹ˆà¸²à¸ˆà¸£à¸´à¸‡à¸à¸±à¸šà¸„à¹ˆà¸²à¸—à¸³à¸™à¸²à¸¢ â€œà¸¡à¸µà¸„à¹ˆà¸²à¸™à¹‰à¸­à¸¢à¸—à¸µà¹ˆà¸ªà¸¸à¸”â€

**Loss Function (à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ªà¸¹à¸à¹€à¸ªà¸µà¸¢):**
\[
L = \sum_{i=1}^{n} (y_i - (m x_i + b))^2
\]

> à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢à¸‚à¸­à¸‡à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰:  
> à¸—à¸³à¹ƒà¸«à¹‰ \( L \) à¸¡à¸µà¸„à¹ˆà¸²à¸™à¹‰à¸­à¸¢à¸—à¸µà¹ˆà¸ªà¸¸à¸” (minimize loss)

---

## âš™ï¸ Gradient Descent (à¸à¸²à¸£à¹„à¸¥à¹ˆà¸£à¸°à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸Šà¸±à¸™)

à¹ƒà¸™à¸ªà¸–à¸´à¸•à¸´, Machine Learning, à¹à¸¥à¸° Data Science â€”  
**Optimization (à¸à¸²à¸£à¸«à¸²à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸—à¸µà¹ˆà¸ªà¸¸à¸”)** à¹€à¸›à¹‡à¸™à¸«à¸±à¸§à¹ƒà¸ˆà¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡à¸«à¸¥à¸²à¸¢à¸­à¸±à¸¥à¸à¸­à¸£à¸´à¸—à¸¶à¸¡

Gradient Descent à¸„à¸·à¸­à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£ **à¸›à¸£à¸±à¸šà¸„à¹ˆà¸²à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡**  
à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ Loss Function à¸¥à¸”à¸¥à¸‡à¸ˆà¸™à¸–à¸¶à¸‡à¸ˆà¸¸à¸”à¸•à¹ˆà¸³à¸ªà¸¸à¸” (minimum)

---

### ðŸ”¹ à¸ªà¸¹à¸•à¸£à¸à¸²à¸£à¸­à¸±à¸›à¹€à¸”à¸•à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ

\[
\theta := \theta - \alpha \frac{\partial L}{\partial \theta}
\]

à¹‚à¸”à¸¢à¸—à¸µà¹ˆ  
- \( \theta \): à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œ (à¹€à¸Šà¹ˆà¸™ m à¸«à¸£à¸·à¸­ b)  
- \( \alpha \): à¸„à¹ˆà¸²à¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰ (Learning rate)  
- \( \frac{\partial L}{\partial \theta} \): à¸­à¸™à¸¸à¸žà¸±à¸™à¸˜à¹Œà¸‚à¸­à¸‡ Loss Function à¸•à¹ˆà¸­ \(\theta\)

---

### ðŸ”¹ à¹ƒà¸™à¸à¸£à¸“à¸µ Linear Regression:
\[
m := m - \alpha \frac{\partial L}{\partial m}
\quad,\quad
b := b - \alpha \frac{\partial L}{\partial b}
\]

à¹‚à¸”à¸¢à¸—à¸µà¹ˆ
\[
\frac{\partial L}{\partial m} = -\frac{2}{n} \sum_{i=1}^{n} x_i (y_i - (m x_i + b))
\]
\[
\frac{\partial L}{\partial b} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - (m x_i + b))
\]

> ðŸ” à¸—à¸³à¸‹à¹‰à¸³à¸«à¸¥à¸²à¸¢à¸£à¸­à¸š (iterations)  
> à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸ˆà¸°à¹„à¸”à¹‰à¸„à¹ˆà¸² \(m, b\) à¸—à¸µà¹ˆà¹€à¸‚à¹‰à¸²à¹ƒà¸à¸¥à¹‰à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸£à¸·à¹ˆà¸­à¸¢ à¹†

---

### âš–ï¸ à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¸­à¸‡ Gradient Descent

| à¸›à¸£à¸°à¹€à¸ à¸— | à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢ | à¸¥à¸±à¸à¸©à¸“à¸°à¹€à¸”à¹ˆà¸™ |
|:--|:--|:--|
| **Batch Gradient Descent** | à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹ƒà¸™à¸à¸²à¸£à¸­à¸±à¸›à¹€à¸”à¸•à¹à¸•à¹ˆà¸¥à¸°à¸„à¸£à¸±à¹‰à¸‡ | à¹€à¸ªà¸–à¸µà¸¢à¸£à¹à¸•à¹ˆà¸Šà¹‰à¸² |
| **Stochastic Gradient Descent (SGD)** | à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸žà¸µà¸¢à¸‡ 1 à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¸£à¸­à¸š | à¹€à¸£à¹‡à¸§à¸‚à¸¶à¹‰à¸™à¹à¸•à¹ˆà¸¡à¸µ noise |
| **Mini-batch Gradient Descent** | à¹ƒà¸Šà¹‰à¸à¸¥à¸¸à¹ˆà¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¢à¹ˆà¸­à¸¢ | à¸ªà¸¡à¸”à¸¸à¸¥à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ |

---

## ðŸ§ª Workshop: Gradient Descent

**à¹‚à¸ˆà¸—à¸¢à¹Œ:**
> à¹ƒà¸Šà¹‰ Least Square Estimation à¹à¸¥à¸° Gradient Descent  
> à¹€à¸žà¸·à¹ˆà¸­à¸«à¸²à¸„à¹ˆà¸² *slope (m)* à¹à¸¥à¸° *intercept (b)* à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹ƒà¸«à¹‰

**à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚:**
1. à¹ƒà¸Šà¹‰ **Learning rate = 0.01**  
2. à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸ˆà¸²à¸à¸„à¹ˆà¸² slope à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸ Least Square Estimation  
3. à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ intercept = 15  
4. à¸—à¸³à¸‹à¹‰à¸³ (iteration) = 10 à¸£à¸­à¸š  
5. à¸§à¸²à¸”à¸à¸£à¸²à¸Ÿà¹€à¸ªà¹‰à¸™à¸•à¸£à¸‡à¸ˆà¸²à¸ slope à¹à¸¥à¸° intercept à¸—à¸µà¹ˆà¹„à¸”à¹‰

---

### ðŸ§­ à¸à¸²à¸£à¸•à¸µà¸„à¸§à¸²à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ

- à¹€à¸¡à¸·à¹ˆà¸­ Loss à¸¥à¸”à¸¥à¸‡à¹€à¸£à¸·à¹ˆà¸­à¸¢ à¹† â†’ Gradient Descent à¸—à¸³à¸‡à¸²à¸™à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡  
- à¹€à¸¡à¸·à¹ˆà¸­ Loss à¹à¸à¸§à¹ˆà¸‡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸¥à¸” â†’ à¸•à¹‰à¸­à¸‡à¸›à¸£à¸±à¸šà¸„à¹ˆà¸² learning rate  
- à¹€à¸¡à¸·à¹ˆà¸­à¸„à¹ˆà¸² Gradient = 0 â†’ à¸–à¸¶à¸‡à¸ˆà¸¸à¸”à¸•à¹ˆà¸³à¸ªà¸¸à¸”à¸‚à¸­à¸‡ Loss (Local Minimum)

---

## ðŸ§  Mermaid Diagram â€” à¸ à¸²à¸žà¸£à¸§à¸¡à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¸‚à¸­à¸‡ Gradient Descent

```mermaid
flowchart TD
  A[à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™: à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸‚à¸­à¸‡ m, b] --> B[à¸„à¸³à¸™à¸§à¸“ Loss Function]
  B --> C[à¸«à¸²à¸„à¹ˆà¸² Gradient à¸‚à¸­à¸‡ m à¹à¸¥à¸° b]
  C --> D[à¸­à¸±à¸›à¹€à¸”à¸•à¸„à¹ˆà¸² m,b à¸”à¹‰à¸§à¸¢ Learning rate]
  D --> E{Loss à¸¥à¸”à¸¥à¸‡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ?}
  E -- à¹ƒà¸Šà¹ˆ --> F[à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ (Converged)]
  E -- à¹„à¸¡à¹ˆ --> B
```